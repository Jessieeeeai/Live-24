import os
import json
import time
import re
import datetime
from langchain_openai import ChatOpenAI
from tavily import TavilyClient

# å†å²è®°å½•æ–‡ä»¶
HISTORY_FILE = "topic_history.json"

class CryptoBrain:
    def __init__(self, deepseek_key, tavily_key, topic_scope, persona_prompt, backup_topics, target_domains):
        self.backup_topics = backup_topics
        self.target_domains = target_domains  # ç”¨æˆ·æŒ‡å®šçš„ä¿¡æºåˆ—è¡¨
        self.recent_frameworks = []  # ğŸ”¥ v2.2: æœ€è¿‘ä½¿ç”¨çš„æ¡†æ¶å†å²ï¼ˆæ¡†æ¶å¤šæ ·æ€§æ£€æŸ¥ï¼‰
        self.topic_outlines = {}  # ğŸ”¥ v2.2: è¯é¢˜å¤§çº²ç¼“å­˜
        
        # 1. åˆå§‹åŒ–å¤§è„‘ (DeepSeek)
        if deepseek_key:
            self.llm = ChatOpenAI(
                model="deepseek-chat", 
                api_key=deepseek_key,
                base_url="https://api.deepseek.com",
                temperature=1.2,  # æé«˜åˆ›é€ æ€§
                timeout=120,  # å¢åŠ è¶…æ—¶æ—¶é—´ï¼Œæ”¯æŒæ·±åº¦åˆ†æ
                max_tokens=4000  # ğŸ”¥ æ˜ç¡®è®¾ç½®æœ€å¤§tokenæ•°ï¼Œç¡®ä¿é•¿å†…å®¹ç”Ÿæˆ
            )
        else:
            self.llm = None
            
        # 2. åˆå§‹åŒ–æœç´¢ (Tavily)
        self.tavily = TavilyClient(api_key=tavily_key) if tavily_key else None
        self.topic = topic_scope
        self.persona = persona_prompt
        
        # 3. ğŸ”¥ å®šä¹‰30ç§æ·±åº¦åˆ†ææ¡†æ¶ (v2.2æ‰©å±•ç‰ˆ)
        self.frameworks = {
            "5W1H": {
                "name": "çƒ­ç‚¹è§£è¯»æ¡†æ¶",
                "structure": "äº‹ä»¶æ¦‚è¿°â†’èƒŒæ™¯åˆ†æâ†’å…³é”®äººç‰©â†’æ—¶é—´è„‰ç»œâ†’æ·±å±‚åŸå› â†’å½±å“é¢„æµ‹",
                "é€‚ç”¨": ["çªå‘äº‹ä»¶", "æ–°é—»å¿«è®¯", "å®æ—¶åŠ¨æ€"]
            },
            "PEST": {
                "name": "è¶‹åŠ¿åˆ†ææ¡†æ¶",
                "structure": "æ”¿æ²»å› ç´ â†’ç»æµç¯å¢ƒâ†’ç¤¾ä¼šæ–‡åŒ–â†’æŠ€æœ¯å˜é©â†’ç»¼åˆå½±å“â†’è¶‹åŠ¿åˆ¤æ–­",
                "é€‚ç”¨": ["å¸‚åœºè¶‹åŠ¿", "å®è§‚ç¯å¢ƒ", "é•¿æœŸå˜åŒ–"]
            },
            "MECE": {
                "name": "å•†ä¸šäº‹ä»¶æ¡†æ¶",
                "structure": "é—®é¢˜æ‹†è§£â†’åˆ†ç±»å½’çº³â†’é€å±‚åˆ†æâ†’é€»è¾‘éªŒè¯â†’ç»“è®ºæ•´åˆ",
                "é€‚ç”¨": ["å•†ä¸šå†³ç­–", "æˆ˜ç•¥åˆ†æ", "å¤æ‚é—®é¢˜"]
            },
            "SWOT": {
                "name": "äººç‰©äº‰è®®æ¡†æ¶",
                "structure": "ä¼˜åŠ¿åˆ†æâ†’åŠ£åŠ¿å‰–æâ†’æœºä¼šè¯†åˆ«â†’å¨èƒè¯„ä¼°â†’æˆ˜ç•¥å»ºè®®",
                "é€‚ç”¨": ["äººç‰©è¯„ä»·", "é¡¹ç›®è¯„ä¼°", "ç«äº‰åˆ†æ"]
            },
            "åˆ©ç›Šç›¸å…³è€…": {
                "name": "æ”¿ç­–è§£è¯»æ¡†æ¶",
                "structure": "æ”¿åºœå±‚é¢â†’ä¼ä¸šè§’åº¦â†’æ°‘ä¼—è§†è§’â†’ä¸“å®¶è§‚ç‚¹â†’åª’ä½“ç«‹åœºâ†’ç»¼åˆå¹³è¡¡",
                "é€‚ç”¨": ["æ”¿ç­–å‘å¸ƒ", "ç›‘ç®¡å˜åŒ–", "å…¬å…±äº‹ä»¶"]
            },
            "æ³¢ç‰¹äº”åŠ›": {
                "name": "äº§ä¸šå˜åŒ–æ¡†æ¶",
                "structure": "ç«äº‰å¯¹æ‰‹â†’ä¾›åº”å•†â†’ä¹°æ–¹â†’æ›¿ä»£å“â†’æ½œåœ¨è¿›å…¥è€…â†’è¡Œä¸šå‰æ™¯",
                "é€‚ç”¨": ["è¡Œä¸šåˆ†æ", "ç«äº‰æ ¼å±€", "å¸‚åœºè¿›å…¥"]
            },
            "é‡‘å­—å¡”åŸç†": {
                "name": "ç¤¾ä¼šç°è±¡æ¡†æ¶",
                "structure": "æ ¸å¿ƒè§‚ç‚¹â†’æ”¯æ’‘è®ºæ®â†’å…·ä½“äº‹å®â†’é€»è¾‘æ¨ç†â†’ç»“è®ºå¼ºåŒ–",
                "é€‚ç”¨": ["è§‚ç‚¹è¡¨è¾¾", "è¯´æœæ²Ÿé€š", "æŠ¥å‘Šæ’°å†™"]
            },
            "é—®é¢˜æ ‘": {
                "name": "æ¡ˆä¾‹å¤ç›˜æ¡†æ¶",
                "structure": "æ ¸å¿ƒé—®é¢˜â†’å­é—®é¢˜æ‹†è§£â†’æ ¹å› åˆ†æâ†’è§£å†³æ–¹æ¡ˆâ†’å®æ–½è·¯å¾„",
                "é€‚ç”¨": ["æ¡ˆä¾‹åˆ†æ", "äº‹åå¤ç›˜", "é—®é¢˜è¯Šæ–­"]
            },
            "å†³ç­–çŸ©é˜µ": {
                "name": "å¯¹æ¯”è¯„ä¼°æ¡†æ¶",
                "structure": "é€‰é¡¹åˆ—ä¸¾â†’è¯„åˆ¤æ ‡å‡†â†’æƒé‡åˆ†é…â†’å¾—åˆ†è¯„ä¼°â†’æœ€ä¼˜é€‰æ‹©",
                "é€‚ç”¨": ["å¤šæ–¹æ¡ˆå¯¹æ¯”", "é€‰æ‹©å†³ç­–", "è¯„ä¼°æ’åº"]
            },
            "æƒ…æ™¯åˆ†æ": {
                "name": "æœªæ¥é¢„æµ‹æ¡†æ¶",
                "structure": "å½“å‰çŠ¶æ€â†’é©±åŠ¨å› ç´ â†’å¯èƒ½æƒ…æ™¯â†’æ¦‚ç‡è¯„ä¼°â†’åº”å¯¹ç­–ç•¥",
                "é€‚ç”¨": ["æœªæ¥å±•æœ›", "é£é™©é¢„åˆ¤", "æˆ˜ç•¥è§„åˆ’"]
            },
            # ğŸ”¥ æ–°å¢20ç§æ¡†æ¶ (v2.2)
            "ä»·å€¼é“¾åˆ†æ": {
                "name": "äº§ä¸šä»·å€¼æ¡†æ¶",
                "structure": "ä¸Šæ¸¸ä¾›åº”â†’æ ¸å¿ƒç”Ÿäº§â†’ä¸‹æ¸¸åˆ†é”€â†’æœåŠ¡æ”¯æŒâ†’ä»·å€¼åˆ›é€ â†’åˆ©æ¶¦åˆ†é…",
                "é€‚ç”¨": ["äº§ä¸šé“¾åˆ†æ", "å•†ä¸šæ¨¡å¼", "æˆæœ¬ç»“æ„"]
            },
            "æŠ€æœ¯æˆç†Ÿåº¦æ›²çº¿": {
                "name": "æŠ€æœ¯åˆ›æ–°æ¡†æ¶",
                "structure": "æŠ€æœ¯è§¦å‘â†’æœŸæœ›è†¨èƒ€â†’å¹»ç­ä½è°·â†’å¤è‹çˆ¬å‡â†’æˆç†Ÿç¨³å®š",
                "é€‚ç”¨": ["æ–°æŠ€æœ¯è¯„ä¼°", "åˆ›æ–°åˆ†æ", "æŠ•èµ„æ—¶æœº"]
            },
            "å•†ä¸šç”»å¸ƒ": {
                "name": "å•†ä¸šæ¨¡å¼æ¡†æ¶",
                "structure": "å®¢æˆ·ç»†åˆ†â†’ä»·å€¼ä¸»å¼ â†’æ¸ é“é€šè·¯â†’å®¢æˆ·å…³ç³»â†’æ”¶å…¥æ¥æºâ†’æ ¸å¿ƒèµ„æºâ†’å…³é”®æ´»åŠ¨â†’åˆä½œä¼™ä¼´â†’æˆæœ¬ç»“æ„",
                "é€‚ç”¨": ["åˆ›ä¸šé¡¹ç›®", "å•†ä¸šæ¨¡å¼", "æˆ˜ç•¥è½¬å‹"]
            },
            "å†²çªçŸ©é˜µ": {
                "name": "äº‰è®®å¯¹ç«‹æ¡†æ¶",
                "structure": "æ­£æ–¹è§‚ç‚¹â†’åæ–¹è§‚ç‚¹â†’æ ¸å¿ƒå†²çªâ†’åˆ©ç›Šåˆ†æ­§â†’å¦¥åç©ºé—´â†’è§£å†³è·¯å¾„",
                "é€‚ç”¨": ["äº‰è®®è¯é¢˜", "å¯¹ç«‹è§‚ç‚¹", "å†²çªåˆ†æ"]
            },
            "è·¯å¾„ä¾èµ–": {
                "name": "å†å²æ¼”è¿›æ¡†æ¶",
                "structure": "åˆå§‹é€‰æ‹©â†’è·¯å¾„é”å®šâ†’å¼ºåŒ–æœºåˆ¶â†’è½¬å‹éšœç¢â†’çªç ´å¯èƒ½",
                "é€‚ç”¨": ["åˆ¶åº¦åˆ†æ", "è¡Œä¸šæƒ¯æ€§", "å˜é©éš¾é¢˜"]
            },
            "ç½‘ç»œæ•ˆåº”": {
                "name": "å¹³å°ç”Ÿæ€æ¡†æ¶",
                "structure": "ç”¨æˆ·å¢é•¿â†’ä»·å€¼æå‡â†’ç½‘ç»œå¯†åº¦â†’ä¸´ç•Œè§„æ¨¡â†’èµ¢å®¶é€šåƒ",
                "é€‚ç”¨": ["å¹³å°ç»æµ", "ç¤¾äº¤ç½‘ç»œ", "åŒè¾¹å¸‚åœº"]
            },
            "ç ´çª—æ•ˆåº”": {
                "name": "ç¤¾ä¼šå¿ƒç†æ¡†æ¶",
                "structure": "åˆå§‹ä¿¡å·â†’å¿ƒç†æš—ç¤ºâ†’è¡Œä¸ºæ‰©æ•£â†’è§„èŒƒå´©æºƒâ†’ç³»ç»Ÿå¤±åº",
                "é€‚ç”¨": ["ç¤¾ä¼šç°è±¡", "ç¾¤ä½“è¡Œä¸º", "ç®¡ç†é—®é¢˜"]
            },
            "é»‘å¤©é¹…äº‹ä»¶": {
                "name": "æç«¯é£é™©æ¡†æ¶",
                "structure": "å¸¸æ€å‡è®¾â†’å¼‚å¸¸å‡ºç°â†’å†²å‡»åˆ†æâ†’è¿é”ååº”â†’åº”å¯¹ç­–ç•¥",
                "é€‚ç”¨": ["çªå‘å±æœº", "ç³»ç»Ÿé£é™©", "å°¾éƒ¨äº‹ä»¶"]
            },
            "é•¿å°¾ç†è®º": {
                "name": "å¸‚åœºåˆ†å¸ƒæ¡†æ¶",
                "structure": "å¤´éƒ¨é›†ä¸­â†’å°¾éƒ¨åˆ†æ•£â†’åˆ©åŸºå¸‚åœºâ†’è§„æ¨¡æ•ˆåº”â†’æ€»é‡å¯¹æ¯”",
                "é€‚ç”¨": ["å¸‚åœºç»†åˆ†", "å°ä¼—éœ€æ±‚", "äº’è”ç½‘ç»æµ"]
            },
            "äºŒå…«å®šå¾‹": {
                "name": "èµ„æºé›†ä¸­æ¡†æ¶",
                "structure": "æ ¸å¿ƒ20%â†’è´¡çŒ®80%â†’èµ„æºé…ç½®â†’ä¼˜å…ˆçº§æ’åºâ†’æ•ˆç‡ä¼˜åŒ–",
                "é€‚ç”¨": ["èµ„æºåˆ†é…", "æ•ˆç‡åˆ†æ", "é‡ç‚¹çªç ´"]
            },
            "é©¬æ–¯æ´›éœ€æ±‚": {
                "name": "ç”¨æˆ·éœ€æ±‚æ¡†æ¶",
                "structure": "ç”Ÿç†éœ€æ±‚â†’å®‰å…¨éœ€æ±‚â†’ç¤¾äº¤éœ€æ±‚â†’å°Šé‡éœ€æ±‚â†’è‡ªæˆ‘å®ç°",
                "é€‚ç”¨": ["ç”¨æˆ·åˆ†æ", "äº§å“è®¾è®¡", "æ¶ˆè´¹è¡Œä¸º"]
            },
            "åˆ›æ–°æ‰©æ•£": {
                "name": "ä¼ æ’­é‡‡çº³æ¡†æ¶",
                "structure": "åˆ›æ–°è€…â†’æ—©æœŸé‡‡çº³â†’æ—©æœŸå¤§ä¼—â†’æ™šæœŸå¤§ä¼—â†’è½åè€…",
                "é€‚ç”¨": ["äº§å“æ¨å¹¿", "å¸‚åœºæ¸—é€", "ç”¨æˆ·å¢é•¿"]
            },
            "å›šå¾’å›°å¢ƒ": {
                "name": "åšå¼ˆè®ºæ¡†æ¶",
                "structure": "ä¸ªä½“ç†æ€§â†’é›†ä½“å›°å¢ƒâ†’ä¿¡ä»»ç¼ºå¤±â†’åˆä½œéšœç¢â†’åˆ¶åº¦è®¾è®¡",
                "é€‚ç”¨": ["ç«äº‰ç­–ç•¥", "åˆä½œé—®é¢˜", "åˆ¶åº¦åˆ†æ"]
            },
            "é›¶å’Œåšå¼ˆ": {
                "name": "ç«äº‰å¯¹æŠ—æ¡†æ¶",
                "structure": "åŒæ–¹å¯¹ç«‹â†’åˆ©ç›Šäº’æ–¥â†’ç­–ç•¥åšå¼ˆâ†’å‡è¡¡ç‚¹â†’è¾“èµ¢åˆ†æ",
                "é€‚ç”¨": ["ç«äº‰å…³ç³»", "èµ„æºäº‰å¤º", "æ”¿æ²»æ–—äº‰"]
            },
            "é£è½®æ•ˆåº”": {
                "name": "å¢é•¿å¾ªç¯æ¡†æ¶",
                "structure": "åˆå§‹æŠ•å…¥â†’å°æ­¥ç§¯ç´¯â†’åŠ¨èƒ½å¢å¼ºâ†’åŠ é€Ÿå¢é•¿â†’è‡ªæˆ‘å¼ºåŒ–",
                "é€‚ç”¨": ["ä¼ä¸šå¢é•¿", "å¤åˆ©æ•ˆåº”", "æˆ˜ç•¥æ‰§è¡Œ"]
            },
            "è·¨è¶Šé¸¿æ²Ÿ": {
                "name": "å¸‚åœºè·¨è¶Šæ¡†æ¶",
                "structure": "æ—©æœŸå¸‚åœºâ†’é¸¿æ²Ÿéšœç¢â†’ä¸»æµå¸‚åœºâ†’è·¨è¶Šç­–ç•¥â†’è§„æ¨¡åŒ–",
                "é€‚ç”¨": ["äº§å“å¸‚åœºåŒ–", "å¢é•¿ç“¶é¢ˆ", "æˆ˜ç•¥è½¬å‹"]
            },
            "æŠ¤åŸæ²³ç†è®º": {
                "name": "ç«äº‰ä¼˜åŠ¿æ¡†æ¶",
                "structure": "è§„æ¨¡ä¼˜åŠ¿â†’æˆæœ¬ä¼˜åŠ¿â†’å“ç‰Œä¼˜åŠ¿â†’ç½‘ç»œæ•ˆåº”â†’è½¬æ¢æˆæœ¬",
                "é€‚ç”¨": ["ç«äº‰åˆ†æ", "æŠ•èµ„ç ”ç©¶", "æˆ˜ç•¥å®šä½"]
            },
            "å®šä½ç†è®º": {
                "name": "å“ç‰Œå®šä½æ¡†æ¶",
                "structure": "å¿ƒæ™ºèµ„æºâ†’ç«äº‰ä½ç½®â†’å·®å¼‚åŒ–â†’èšç„¦ç­–ç•¥â†’å“ç‰Œè®¤çŸ¥",
                "é€‚ç”¨": ["å“ç‰Œæˆ˜ç•¥", "å¸‚åœºå®šä½", "è¥é”€ç­–ç•¥"]
            },
            "åŒå› ç´ ç†è®º": {
                "name": "åŠ¨æœºæ¿€åŠ±æ¡†æ¶",
                "structure": "ä¿å¥å› ç´ â†’æ¿€åŠ±å› ç´ â†’æ»¡æ„åº¦â†’ä¸æ»¡æ„åº¦â†’ç»¼åˆæ•ˆåº”",
                "é€‚ç”¨": ["äººæ‰ç®¡ç†", "æ¿€åŠ±æœºåˆ¶", "ç»„ç»‡è¡Œä¸º"]
            },
            "ä¸´ç•Œç‚¹ç†è®º": {
                "name": "æ‹ç‚¹çªå˜æ¡†æ¶",
                "structure": "æ¸è¿›ç´¯ç§¯â†’ä¸´ç•Œé˜ˆå€¼â†’çªå˜è·ƒè¿â†’æ–°å‡è¡¡â†’ä¸å¯é€†æ€§",
                "é€‚ç”¨": ["è¶‹åŠ¿é¢„åˆ¤", "ç¤¾ä¼šå˜é©", "å¸‚åœºè½¬æŠ˜"]
            }
        }

    def _calculate_viral_potential(self, news_item):
        """
        ğŸ”¥ Step 2: è®¡ç®—çˆ†ç«æ½œåŠ›è¯„åˆ†
        è¯„åˆ†ç»´åº¦ï¼šæ–°é²œåº¦(30%) + äº‰è®®æ€§(25%) + å—ä¼—è¦†ç›–(20%) + ä¼ æ’­é€Ÿåº¦(15%) + æƒ…ç»ªå¼ºåº¦(10%)
        """
        score = 0
        # ğŸ”¥ FIX: å…¼å®¹ Tavily API çš„ä¸åŒå­—æ®µå (title/name, content/snippet)
        title = news_item.get('title') or news_item.get('name') or ''
        content_text = news_item.get('content') or news_item.get('snippet') or news_item.get('description') or ''
        content = (title + ' ' + content_text).lower()
        
        # 1. æ–°é²œåº¦ (30åˆ†) - åŸºäºå‘å¸ƒæ—¶é—´
        published_at = news_item.get('published_date', '')
        if published_at:
            # ç®€å•å¤„ç†ï¼šå¦‚æœæœ‰ä»Šå¤©çš„å…³é”®è¯ï¼Œå¾—é«˜åˆ†
            if 'today' in published_at or datetime.datetime.now().strftime('%Y-%m-%d') in published_at:
                score += 30
            else:
                score += 15
        else:
            score += 20  # é»˜è®¤åˆ†
        
        # 2. äº‰è®®æ€§ (25åˆ†) - å…³é”®è¯æ£€æµ‹
        controversial_words = ['äº‰è®®', 'controversial', 'å´©ç›˜', 'crash', 'æš´æ¶¨', 'surge', 
                              'è¯ˆéª—', 'scam', 'èµ·è¯‰', 'lawsuit', 'ç›‘ç®¡', 'regulation']
        controversy_score = sum(5 for word in controversial_words if word in content)
        score += min(25, controversy_score)
        
        # 3. å—ä¼—è¦†ç›–é¢ (20åˆ†) - è¯é¢˜çƒ­åº¦
        hot_topics = ['bitcoin', 'btc', 'ethereum', 'eth', 'ai', 'solana', 'sec', 'binance']
        coverage_score = sum(5 for topic in hot_topics if topic in content)
        score += min(20, coverage_score)
        
        # 4. ä¼ æ’­é€Ÿåº¦ (15åˆ†) - æ¥æºæƒå¨æ€§
        trusted_sources = ['coindesk', 'cointelegraph', 'theblock', 'decrypt']
        source = news_item.get('url', '').lower()
        if any(s in source for s in trusted_sources):
            score += 15
        else:
            score += 8
        
        # 5. æƒ…ç»ªå¼ºåº¦ (10åˆ†) - å¼ºæƒ…æ„Ÿè¯
        emotion_words = ['æƒŠäºº', 'shocking', 'å²æ— å‰ä¾‹', 'unprecedented', 'é‡å¤§', 'major']
        emotion_score = sum(3 for word in emotion_words if word in content)
        score += min(10, emotion_score)
        
        return score

    def _match_framework(self, news_item):
        """
        ğŸ”¥ Step 3-4: æ™ºèƒ½æ¡†æ¶åŒ¹é…
        æ ¹æ®æ–°é—»ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä½³åˆ†ææ¡†æ¶
        """
        # ğŸ”¥ FIX: å…¼å®¹ä¸åŒå­—æ®µå
        title = news_item.get('title') or news_item.get('name') or ''
        content_text = news_item.get('content') or news_item.get('snippet') or news_item.get('description') or ''
        content = (title + ' ' + content_text).lower()
        
        # å…³é”®è¯ â†’ æ¡†æ¶æ˜ å°„
        framework_keywords = {
            "5W1H": ["çªå‘", "breaking", "åˆšåˆš", "just", "æœ€æ–°", "latest"],
            "PEST": ["è¶‹åŠ¿", "trend", "å±•æœ›", "outlook", "æœªæ¥", "future"],
            "MECE": ["åˆ†æ", "analysis", "æ·±åº¦", "deep dive", "è¯¦è§£"],
            "SWOT": ["äººç‰©", "ceo", "åˆ›å§‹äºº", "founder", "é¡¹ç›®", "project"],
            "åˆ©ç›Šç›¸å…³è€…": ["æ”¿ç­–", "policy", "ç›‘ç®¡", "regulation", "æ³•æ¡ˆ", "law"],
            "æ³¢ç‰¹äº”åŠ›": ["ç«äº‰", "competition", "å¸‚åœº", "market", "è¡Œä¸š", "industry"],
            "é‡‘å­—å¡”åŸç†": ["è§‚ç‚¹", "opinion", "è¯„è®º", "commentary"],
            "é—®é¢˜æ ‘": ["å¤±è´¥", "failure", "å´©ç›˜", "crash", "å¤ç›˜", "post-mortem"],
            "å†³ç­–çŸ©é˜µ": ["å¯¹æ¯”", "comparison", "é€‰æ‹©", "choice", "vs"],
            "æƒ…æ™¯åˆ†æ": ["é¢„æµ‹", "prediction", "å±•æœ›", "forecast", "å¯èƒ½", "potential"]
        }
        
        # ç»Ÿè®¡æ¯ä¸ªæ¡†æ¶çš„åŒ¹é…åº¦
        match_scores = {}
        for framework, keywords in framework_keywords.items():
            match_scores[framework] = sum(1 for kw in keywords if kw in content)
        
        # é€‰æ‹©åŒ¹é…åº¦æœ€é«˜çš„æ¡†æ¶
        best_framework = max(match_scores.items(), key=lambda x: x[1])[0]
        
        # å¦‚æœæ‰€æœ‰æ¡†æ¶å¾—åˆ†éƒ½æ˜¯0ï¼Œé»˜è®¤ä½¿ç”¨5W1H
        if match_scores[best_framework] == 0:
            best_framework = "5W1H"
        
        return best_framework

    def _collect_evidence(self, topic, news_item):
        """
        ğŸ”¥ Step 5: è¯æ®æ”¶é›†ä¸ä¸¥æ ¼ç­›é€‰
        å¹¿æ³›æ”¶é›†æ­£åé¢è¯æ® â†’ æ—¶æ•ˆæ€§æ£€æŸ¥ â†’ é€»è¾‘æ€§éªŒè¯ â†’ å¯é æ€§è¯„ä¼°
        """
        print("ğŸ“š Step 5: æ”¶é›†å’Œç­›é€‰è¯æ®...")
        
        # ğŸ”¥ FIX: å…¼å®¹ä¸åŒå­—æ®µå
        title = news_item.get('title') or news_item.get('name') or ''
        
        # 1. å¹¿æ³›æ”¶é›†ï¼ˆæ­£åé¢ï¼‰
        try:
            search_query = f"{topic} {title}"
            evidence_pool = self.tavily.search(
                query=search_query,
                search_depth="advanced",
                max_results=10,
                days=3  # æ‰©å¤§åˆ°3å¤©ï¼Œç¡®ä¿è¶³å¤Ÿè¯æ®
            )
            raw_evidence = evidence_pool.get("results", [])
        except Exception as e:
            print(f"âš ï¸ è¯æ®æ”¶é›†å¤±è´¥: {e}")
            raw_evidence = [news_item]  # å¤±è´¥æ—¶è‡³å°‘ç”¨åŸæ–°é—»
        
        # 2. æ—¶æ•ˆæ€§æ£€æŸ¥ï¼ˆåˆ é™¤è¿‡æ—¶ä¿¡æ¯ï¼‰
        valid_evidence = []
        for e in raw_evidence:
            # ç®€å•æ£€æŸ¥ï¼šæœ‰å‘å¸ƒæ—¥æœŸä¸”ä¸æ˜¯å¤ªæ—§
            pub_date = e.get('published_date', '')
            if pub_date or 'http' in e.get('url', ''):
                valid_evidence.append(e)
        
        # 3. é€»è¾‘æ€§éªŒè¯ï¼ˆç²—ç­›ï¼‰
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ£€æŸ¥å†…å®¹ç›¸å…³æ€§
        logical_evidence = valid_evidence[:8]  # å–å‰8ä¸ªæœ€ç›¸å…³çš„
        
        # 4. å¯é æ€§è¯„ä¼°ï¼ˆæ£€æŸ¥æ¥æºï¼‰
        reliable_evidence = []
        for e in logical_evidence:
            url = e.get('url', '').lower()
            # ä¼˜å…ˆæ¥è‡ªå¯ä¿¡æº
            is_trusted = any(source in url for source in ['coindesk', 'cointelegraph', 'theblock', 'decrypt', 'reuters', 'bloomberg'])
            if is_trusted or len(reliable_evidence) < 3:  # ç¡®ä¿è‡³å°‘3æ¡
                reliable_evidence.append(e)
        
        print(f"âœ… è¯æ®ç­›é€‰å®Œæˆ: åŸå§‹{len(raw_evidence)}æ¡ â†’ æœ‰æ•ˆ{len(valid_evidence)}æ¡ â†’ å¯é {len(reliable_evidence)}æ¡")
        return reliable_evidence

    def _organize_content(self, evidence, framework, news_item):
        """
        ğŸ”¥ Step 6: å†…å®¹ç»„ç»‡ï¼ˆé‡‘å­—å¡”åŸç†ï¼‰
        æŒ‰æ¡†æ¶ç»“æ„ç»„ç»‡ç´ æï¼Œè°ƒèŠ‚èŠ‚å¥ï¼Œæ§åˆ¶ç¯‡å¹…
        """
        print(f"ğŸ“ Step 6: æŒ‰ {framework} æ¡†æ¶ç»„ç»‡å†…å®¹...")
        
        framework_info = self.frameworks.get(framework, self.frameworks["5W1H"])
        
        # ğŸ”¥ FIX: å…¼å®¹ä¸åŒå­—æ®µå
        title = news_item.get('title') or news_item.get('name') or ''
        content_text = news_item.get('content') or news_item.get('snippet') or news_item.get('description') or ''
        url = news_item.get('url') or ''
        
        # æ„å»ºç»“æ„åŒ–çš„ç´ æåŒ…
        organized = {
            "æ¡†æ¶åç§°": framework_info["name"],
            "ç»“æ„": framework_info["structure"],
            "ä¸»æ–°é—»": {
                "æ ‡é¢˜": title,
                "å†…å®¹": content_text,
                "æ¥æº": url
            },
            "æ”¯æ’‘è¯æ®": [
                {
                    "æ ‡é¢˜": e.get('title') or e.get('name') or '',
                    "æ‘˜è¦": (e.get('content') or e.get('snippet') or e.get('description') or '')[:200],
                    "æ¥æº": e.get('url') or ''
                }
                for e in evidence[:3]  # æœ€å¤š3æ¡æ”¯æ’‘
            ]
        }
        
        return organized

    def _check_duplication(self, new_topic):
        """
        å»é‡æœºåˆ¶ï¼šé¿å…çŸ­æ—¶é—´å†…é‡å¤è®²åŒä¸€ä¸ªæ–°é—»
        """
        # ğŸ”¥ FIX: å¤„ç†ç©ºæ ‡é¢˜æƒ…å†µ
        if not new_topic or new_topic.strip() == '':
            print("âš ï¸ æ ‡é¢˜ä¸ºç©ºï¼Œè·³è¿‡å»é‡æ£€æŸ¥")
            return False
        try:
            if not os.path.exists(HISTORY_FILE):
                with open(HISTORY_FILE, "w") as f:
                    json.dump([{"topic": new_topic, "time": time.time()}], f)
                return False
            
            with open(HISTORY_FILE, "r") as f: 
                history = json.load(f)
            
            # æ¸…ç†è¶…è¿‡5å°æ—¶çš„æ—§è®°å½•
            current_time = time.time()
            valid_history = [h for h in history if current_time - h['time'] < 5 * 3600]
            
            # æŸ¥é‡ï¼ˆå…³é”®è¯åŒ¹é… + ç›¸ä¼¼åº¦ï¼‰
            is_dup = False
            for h in valid_history:
                if h['topic'] in new_topic or new_topic in h['topic']:
                    is_dup = True
                    break
                # è¯æ±‡ç›¸ä¼¼åº¦æ£€æµ‹
                old_words = set(re.findall(r'\w+', h['topic'].lower()))
                new_words = set(re.findall(r'\w+', new_topic.lower()))
                if len(old_words & new_words) / max(len(new_words), 1) > 0.5:
                    is_dup = True
                    break
            
            # å¦‚æœä¸é‡å¤ï¼Œæ›´æ–°æ–‡ä»¶
            if not is_dup:
                valid_history.append({"topic": new_topic, "time": current_time})
                with open(HISTORY_FILE, "w") as f: 
                    json.dump(valid_history, f, ensure_ascii=False, indent=2)
                print(f"âœ… æ–°è¯é¢˜å·²è®°å½•: {new_topic[:50]}...")
            else:
                print(f"âš ï¸ è¯é¢˜é‡å¤ï¼Œè·³è¿‡: {new_topic[:50]}...")
            
            return is_dup
        except Exception as e:
            print(f"âš ï¸ å»é‡æ£€æŸ¥å¤±è´¥: {e}")
            return False

    def _clean_text(self, text):
        """
        ğŸ”¥ å¼ºåŠ›å»åºŸè¯æ­£åˆ™æ¸…æ´—å™¨ï¼ˆæ‰©å±•ç‰ˆ + é˜²æ­¢TTSè¯»å‡ºæ ¼å¼æ ‡è®° + å»é™¤æ¡†æ¶å…ƒä¿¡æ¯ï¼‰
        """
        if not text:
            return ""
        
        # 0.1 ğŸ”¥ğŸ”¥ æœ€é«˜ä¼˜å…ˆçº§ï¼šå»é™¤æ–‡ç« å¼€å¤´çš„æ¡†æ¶åç§°å’Œå…ƒä¿¡æ¯
        framework_patterns = [
            r'^.*?5W1H.*?[:ï¼š\n]',
            r'^.*?PEST.*?[:ï¼š\n]',
            r'^.*?SWOT.*?[:ï¼š\n]',
            r'^.*?MECE.*?[:ï¼š\n]',
            r'^.*?æ¡†æ¶.*?[:ï¼š\n]',
            r'^.*?åˆ†æ.*?[:ï¼š\n]',
            r'^.*?æ³¢ç‰¹äº”åŠ›.*?[:ï¼š\n]',
            r'^.*?é‡‘å­—å¡”åŸç†.*?[:ï¼š\n]',
            r'^.*?åˆ©ç›Šç›¸å…³è€….*?[:ï¼š\n]',
            r'^.*?é—®é¢˜æ ‘.*?[:ï¼š\n]',
            r'^.*?å†³ç­–çŸ©é˜µ.*?[:ï¼š\n]',
            r'^.*?æƒ…æ™¯åˆ†æ.*?[:ï¼š\n]',
            r'^ã€.*?ã€‘',  # Remove content in ã€ã€‘brackets at start
            r'^\s*æˆ‘é€‰æ‹©äº†.*?[:ï¼šã€‚\n]',
            r'^\s*ä½¿ç”¨.*?æ¡†æ¶.*?[:ï¼šã€‚\n]',
            r'^\s*é‡‡ç”¨.*?åˆ†æ.*?[:ï¼šã€‚\n]',
        ]
        for pattern in framework_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.MULTILINE)
        
        # 0.2 ğŸ”¥ æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸­æ–‡å­—ç¬¦ï¼Œä»é‚£é‡Œå¼€å§‹ï¼ˆå»é™¤å¼€å¤´çš„è‹±æ–‡å­—æ¯å’Œæ•°å­—ï¼‰
        match = re.search(r'[\u4e00-\u9fff]', text)
        if match:
            first_chinese_pos = match.start()
            # æ£€æŸ¥ç¬¬ä¸€ä¸ªä¸­æ–‡å­—ç¬¦ä¹‹å‰æ˜¯å¦å…¨æ˜¯è‹±æ–‡å­—æ¯ã€æ•°å­—ã€ç¬¦å·
            before_chinese = text[:first_chinese_pos].strip()
            if before_chinese and re.match(r'^[a-zA-Z0-9\s\.\-_:#\[\]ã€ã€‘\(\)ï¼ˆï¼‰]+$', before_chinese):
                # å¦‚æœå¼€å¤´éƒ¨åˆ†æ²¡æœ‰å®è´¨å†…å®¹ï¼Œå°±ä»ç¬¬ä¸€ä¸ªä¸­æ–‡å­—ç¬¦å¼€å§‹
                text = text[first_chinese_pos:]
        
        # 0.3 ğŸ”¥ æœ€ä¼˜å…ˆï¼šå»é™¤è¡Œé¦–çš„æ•°å­—åºå·å’Œå­—æ¯æ ‡è®°ï¼ˆé˜²æ­¢TTSæœ—è¯»ï¼‰
        text = re.sub(r'^\s*[\da-zA-Z]+[\.\)ã€ï¼š:]\s*', '', text, flags=re.MULTILINE)
        text = re.sub(r'^\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+[\.\)ã€ï¼š:]\s*', '', text, flags=re.MULTILINE)
        
        # 1. å»æ‰å‰§æœ¬æ ‡è®°å’Œæ‹¬å·å†…å®¹
        text = re.sub(r"[\(\[\ã€<].*?[\)\]\ã€‘>]", "", text)
        
        # 2. å»æ‰ Markdown æ ¼å¼æ ‡è®°
        text = text.replace("*", "").replace("#", "").replace("`", "")
        text = text.replace("_", "").replace("~", "").replace("-", "")
        
        # 2.5 ğŸ”¥ å»é™¤å¸¸è§çš„ç»“æ„æ ‡è®°å’Œæ¡†æ¶åç§°
        structure_markers = [
            "æ ‡é¢˜ï¼š", "å¼•è¨€ï¼š", "èƒŒæ™¯ï¼š", "åˆ†æï¼š", "ç»“è®ºï¼š",
            "æ­£æ–‡ï¼š", "å¼€å¤´ï¼š", "ç»“å°¾ï¼š", "æ‘˜è¦ï¼š", "å¯¼è¯­ï¼š",
            "ç¬¬ä¸€éƒ¨åˆ†", "ç¬¬äºŒéƒ¨åˆ†", "ç¬¬ä¸‰éƒ¨åˆ†", "ç¬¬å››éƒ¨åˆ†",
            "Step 1", "Step 2", "Step 3", "Step 4", "Step 5",
            "5W1Hæ¡†æ¶", "PESTæ¡†æ¶", "SWOTæ¡†æ¶", "MECEæ¡†æ¶",
            "æ³¢ç‰¹äº”åŠ›", "é‡‘å­—å¡”åŸç†", "åˆ©ç›Šç›¸å…³è€…", "é—®é¢˜æ ‘", "å†³ç­–çŸ©é˜µ", "æƒ…æ™¯åˆ†æ",
            "æˆ‘é€‰æ‹©", "ä½¿ç”¨æ¡†æ¶", "é‡‡ç”¨åˆ†æ", "åŸºäºæ¡†æ¶",
            "ä¸€ã€", "äºŒã€", "ä¸‰ã€", "å››ã€", "äº”ã€",
            "1ã€", "2ã€", "3ã€", "4ã€", "5ã€"
        ]
        for marker in structure_markers:
            text = text.replace(marker, "")
        
        # 3. å»æ‰ AI ä¹ æƒ¯æ€§åºŸè¯ï¼ˆæ‰©å±•åˆ—è¡¨ + æ¡†æ¶å…ƒè¯­è¨€ï¼‰
        bad_phrases = [
            "å¥½çš„å¤§æ¼‚äº®", "æ²¡é—®é¢˜", "å¥½çš„", "ç»¼ä¸Šæ‰€è¿°", "æ€»ä¹‹", "æ€»è€Œè¨€ä¹‹",
            "ä¸»æŒäºº", "Let's go", "å„ä½å¬ä¼—", "å¤§å®¶å¥½", "è§‚ä¼—æœ‹å‹ä»¬",
            "æ¥ä¸‹æ¥", "é‚£ä¹ˆ", "é¦–å…ˆ", "å…¶æ¬¡", "æœ€å", "ç„¶å",
            "å€¼å¾—æ³¨æ„çš„æ˜¯", "éœ€è¦æŒ‡å‡ºçš„æ˜¯", "æˆ‘ä»¬å¯ä»¥çœ‹åˆ°", "å¯ä»¥å‘ç°",
            "æ ¹æ®ä»¥ä¸Šåˆ†æ", "é€šè¿‡åˆ†æ", "ç»¼åˆæ¥çœ‹",
            "éŸ³æ•ˆ", "èƒŒæ™¯éŸ³ä¹", "æŒå£°", "ç¬‘å£°",
            "æˆ‘é€‰æ‹©", "æˆ‘è®¤ä¸º", "æˆ‘è§‰å¾—", "è®©æˆ‘ä»¬",
            "æ¬¢è¿æ”¶å¬", "æ„Ÿè°¢æ”¶çœ‹", "ä¸‹æœŸå†è§",
            "æˆ‘é€‰æ‹©äº†", "æˆ‘ä»¬é€‰æ‹©", "æˆ‘ä»¬ä½¿ç”¨", "æˆ‘ä»¬é‡‡ç”¨",
            "ä½¿ç”¨äº†", "é‡‡ç”¨äº†", "åº”ç”¨äº†", "åŸºäº",
            "æ¡†æ¶è¿›è¡Œåˆ†æ", "æ¡†æ¶æ¥åˆ†æ", "åˆ†ææ¡†æ¶"
        ]
        for phrase in bad_phrases:
            text = text.replace(phrase, "")
        
        # 4. å»é™¤ä¸é€‚åˆæœ—è¯»çš„æ ‡ç‚¹ç¬¦å·
        text = re.sub(r'["""''ã€Œã€ã€ã€ï¼ˆï¼‰()ã€ã€‘ã€Šã€‹<>\[\]]', '', text)  # noqa: W605
        
        # 5. è§„èŒƒåŒ–æ ‡ç‚¹
        text = re.sub(r'[ï¼Œ,]{2,}', 'ï¼Œ', text)
        text = re.sub(r'[ã€‚.]{2,}', 'ã€‚', text)
        text = re.sub(r'[ï¼!]{2,}', 'ï¼', text)
        text = re.sub(r'[ï¼Ÿ?]{2,}', 'ï¼Ÿ', text)
        
        # 6. å»é™¤å¤šä½™ç©ºè¡Œå’Œé¦–å°¾ç©ºæ ¼
        lines = [line.strip() for line in text.split("\n") if line.strip()]
        text = "\n".join(lines)
        
        # 7. ğŸ”¥ å†æ¬¡å»é™¤å¯èƒ½æ®‹ç•™çš„åºå·ï¼ˆæ›´æ¿€è¿›ï¼‰
        text = re.sub(r'^\s*[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€\.\s]+', '', text, flags=re.MULTILINE)
        text = re.sub(r'[a-zA-Z]{1,2}\.\s', '', text)  # å»é™¤ "a. " "A. " ç­‰
        
        # 8. ğŸ”¥ å»é™¤è¡Œé¦–çš„å†’å·ï¼ˆå¦‚æœå•ç‹¬å‡ºç°ï¼‰
        text = re.sub(r'^[:ï¼š]\s*', '', text, flags=re.MULTILINE)
        
        # 9. ğŸ”¥ğŸ”¥ æœ€ç»ˆæ£€æŸ¥ï¼šå¦‚æœå¼€å¤´ä»æœ‰éä¸­æ–‡å†…å®¹ï¼ˆè‹±æ–‡å•è¯/æ•°å­—ï¼‰ï¼Œå¼ºåˆ¶ä»ç¬¬ä¸€å¥è¯å¼€å§‹
        lines = text.split('\n')
        cleaned_lines = []
        found_real_content = False
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # å¦‚æœè¿˜æ²¡æ‰¾åˆ°çœŸæ­£çš„å†…å®¹ï¼Œæ£€æŸ¥è¿™ä¸€è¡Œ
            if not found_real_content:
                # å¦‚æœè¿™è¡Œä»¥ä¸­æ–‡å¼€å¤´ï¼Œä¸”ä¸åŒ…å«æ¡†æ¶å…³é”®è¯ï¼Œè®¤ä¸ºæ˜¯çœŸæ­£çš„å†…å®¹
                if re.match(r'^[\u4e00-\u9fff]', line):
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¡†æ¶å…³é”®è¯
                    framework_keywords = ['æ¡†æ¶', '5W1H', 'PEST', 'SWOT', 'MECE', 'æ³¢ç‰¹äº”åŠ›', 
                                        'é‡‘å­—å¡”', 'åˆ©ç›Šç›¸å…³è€…', 'é—®é¢˜æ ‘', 'å†³ç­–çŸ©é˜µ', 'æƒ…æ™¯åˆ†æ',
                                        'æˆ‘é€‰æ‹©', 'ä½¿ç”¨', 'é‡‡ç”¨', 'åŸºäº']
                    if not any(keyword in line for keyword in framework_keywords):
                        found_real_content = True
                        cleaned_lines.append(line)
                # å¦åˆ™è·³è¿‡è¿™è¡Œï¼ˆå¯èƒ½æ˜¯å…ƒä¿¡æ¯ï¼‰
            else:
                # å·²ç»æ‰¾åˆ°çœŸæ­£çš„å†…å®¹åï¼Œä¿ç•™æ‰€æœ‰è¡Œ
                cleaned_lines.append(line)
        
        text = '\n'.join(cleaned_lines)
        
        return text.strip()

    def _check_semantic_duplication(self, title, threshold=0.6, time_window_hours=24):
        """
        ğŸ”¥ v2.2 - Step 4: è¯­ä¹‰å»é‡æ£€æŸ¥ï¼ˆ60%ç›¸ä¼¼åº¦é˜ˆå€¼ï¼‰
        åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œè€Œéç®€å•å…³é”®è¯åŒ¹é…
        """
        if not os.path.exists(HISTORY_FILE):
            return False  # æ— å†å²ï¼Œä¸é‡å¤
        
        try:
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                history = json.load(f)
        except:
            return False
        
        # æ—¶é—´çª—å£è¿‡æ»¤ï¼ˆ24å°æ—¶å†…ï¼‰
        cutoff_time = time.time() - (time_window_hours * 3600)
        recent_topics = [h for h in history if h.get('timestamp', 0) > cutoff_time]
        
        if not recent_topics:
            return False
        
        # ç®€åŒ–çš„è¯­ä¹‰ç›¸ä¼¼åº¦æ£€æŸ¥ï¼ˆåŸºäºè¯æ±‡é‡å ï¼‰
        title_words = set(title.lower().split())
        for topic in recent_topics:
            old_title = topic.get('topic', '')
            old_words = set(old_title.lower().split())
            
            # è®¡ç®—Jaccardç›¸ä¼¼åº¦
            if len(title_words) == 0 or len(old_words) == 0:
                continue
            
            intersection = len(title_words & old_words)
            union = len(title_words | old_words)
            similarity = intersection / union if union > 0 else 0
            
            if similarity >= threshold:
                print(f"âš ï¸ å»é‡æ£€æµ‹ï¼šä¸ '{old_title[:30]}...' ç›¸ä¼¼åº¦ {similarity:.1%}ï¼ˆé˜ˆå€¼{threshold:.0%}ï¼‰")
                return True
        
        return False
    
    def _check_framework_diversity(self, current_framework):
        """
        ğŸ”¥ v2.2 - Step 6: æ¡†æ¶å¤šæ ·æ€§æ£€æŸ¥
        ç¡®ä¿ä¸é‡å¤ä½¿ç”¨åŒä¸€æ¡†æ¶ï¼Œå¢åŠ å†…å®¹å¤šæ ·æ€§
        """
        if current_framework in self.recent_frameworks[-2:]:  # æœ€è¿‘2æ¬¡
            print(f"âš ï¸ æ¡†æ¶å¤šæ ·æ€§ï¼š{current_framework} æœ€è¿‘ä½¿ç”¨è¿‡ï¼Œå»ºè®®æ›´æ¢")
            # æ‰¾åˆ°ä¸‹ä¸€ä¸ªå¯ç”¨æ¡†æ¶
            for fw in self.frameworks.keys():
                if fw not in self.recent_frameworks[-3:]:
                    print(f"âœ… åˆ‡æ¢åˆ°: {fw}")
                    return fw
        
        # è®°å½•ä½¿ç”¨å†å²ï¼ˆæœ€å¤šä¿ç•™10ä¸ªï¼‰
        self.recent_frameworks.append(current_framework)
        if len(self.recent_frameworks) > 10:
            self.recent_frameworks.pop(0)
        
        return current_framework
    
    def _design_outline(self, topic, framework, news_item):
        """
        ğŸ”¥ v2.2 - Step 7: è®¾è®¡æ–‡ç« å¤§çº²
        åœ¨æ’°å†™å‰å…ˆè§„åˆ’ç»“æ„ï¼Œç¡®ä¿å†…å®¹å……å®
        """
        print("ğŸ“ Step 7: è®¾è®¡å†…å®¹å¤§çº²...")
        
        framework_structure = self.frameworks[framework]['structure']
        sections = framework_structure.split('â†’')
        
        outline = {
            'å¼•è¨€': {'target_chars': 250, 'desc': 'ç®€è¦ä»‹ç»äº‹ä»¶èƒŒæ™¯ï¼Œå¸å¼•å¬ä¼—æ³¨æ„'},
            'ä¸»ä½“': [],
            'ç»“è®º': {'target_chars': 250, 'desc': 'æ€»ç»“åˆ†æï¼Œç»™å‡ºè§‚ç‚¹æˆ–é¢„æµ‹'}
        }
        
        # ä¸ºæ¯ä¸ªæ¡†æ¶ç¯èŠ‚è®¾è®¡å­ç« èŠ‚
        for i, section in enumerate(sections):
            outline['ä¸»ä½“'].append({
                'section': section,
                'target_chars': 300,  # æ¯ç¯èŠ‚ç›®æ ‡300å­—
                'desc': f'è¯¦ç»†åˆ†æ{section}ï¼ŒåŒ…å«äº‹å®ã€æ•°æ®ã€æ¡ˆä¾‹'
            })
        
        total_target = 250 + (len(sections) * 300) + 250
        print(f"ğŸ“Š å¤§çº²è®¾è®¡å®Œæˆï¼šå¼•è¨€(250å­—) + {len(sections)}ä¸ªç¯èŠ‚({len(sections)*300}å­—) + ç»“è®º(250å­—) = çº¦{total_target}å­—")
        
        self.topic_outlines[topic] = outline
        return outline
    
    def _verify_evidence_timeline(self, evidence_list):
        """
        ğŸ”¥ v2.2 - Step 11: éªŒè¯è¯æ®æ—¶é—´çº¿
        ç¡®ä¿ç´ ææ—¶æ•ˆæ€§ï¼Œæ ‡è®°è¿‡æœŸå†…å®¹
        """
        print("â±ï¸ Step 11: éªŒè¯è¯æ®æ—¶é—´çº¿...")
        
        verified = []
        outdated = []
        
        for e in evidence_list:
            pub_date = e.get('published_date', '')
            title = e.get('æ ‡é¢˜') or e.get('title') or e.get('name') or ''
            
            # ç®€å•æ—¶æ•ˆæ€§æ£€æŸ¥
            is_recent = False
            if pub_date:
                # å¦‚æœåŒ…å« today, æœ€è¿‘æ—¥æœŸç­‰
                today = datetime.datetime.now().strftime('%Y-%m-%d')
                yesterday = (datetime.datetime.now() - datetime.timedelta(days=1)).strftime('%Y-%m-%d')
                
                if today in pub_date or yesterday in pub_date or 'today' in pub_date.lower():
                    is_recent = True
            
            if is_recent or not pub_date:  # æ— æ—¥æœŸçš„ä¿ç•™ï¼ˆå¯èƒ½æ˜¯æœ€æ–°ï¼‰
                verified.append(e)
            else:
                outdated.append(title[:30] + '...')
        
        if outdated:
            print(f"âš ï¸ å‘ç°è¿‡æ—¶ç´ æ {len(outdated)} æ¡ï¼š{', '.join(outdated[:3])}")
        
        print(f"âœ… ä¿ç•™æ—¶æ•ˆæ€§è¯æ® {len(verified)}/{len(evidence_list)} æ¡")
        return verified
    
    def _polish_and_audit(self, draft, requirements):
        """
        ğŸ”¥ v2.2 - Step 14: æ¶¦è‰²å’Œå®¡æ ¸ä¼˜åŒ–
        åå¤„ç†ï¼šæ£€æŸ¥å¹¶ä¿®æ­£è´¨é‡é—®é¢˜
        """
        print("ğŸ” Step 14: æ¶¦è‰²å’Œå®¡æ ¸ä¼˜åŒ–...")
        
        issues = []
        
        # 1. å­—æ•°æ£€æŸ¥
        char_count = len(draft)
        if char_count < 1200:
            issues.append(f"å†…å®¹åçŸ­ï¼ˆ{char_count}å­—ï¼‰ï¼Œå»ºè®®æ‰©å……è‡³1500å­—ä»¥ä¸Š")
        
        # 2. ç»“æ„å®Œæ•´æ€§æ£€æŸ¥
        has_opening = any(word in draft[:200] for word in ['ä»Šå¤©', 'æœ€è¿‘', 'è¿™å‡ å¤©', 'å°±åœ¨'])
        has_conclusion = any(word in draft[-200:] for word in ['æ€»ä¹‹', 'æ€»çš„æ¥è¯´', 'æœªæ¥', 'é¢„è®¡', 'ç»¼åˆ'])
        
        if not has_opening:
            issues.append("ç¼ºå°‘å¼•å…¥æ€§å¼€åœº")
        if not has_conclusion:
            issues.append("ç¼ºå°‘æ€»ç»“æ€§ç»“è®º")
        
        # 3. æ·±åº¦åˆ†ææ£€æŸ¥
        analysis_keywords = ['å› ä¸º', 'åŸå› ', 'å½±å“', 'å¯¼è‡´', 'æ„å‘³ç€', 'é¢„è®¡', 'åˆ†æ']
        analysis_count = sum(1 for keyword in analysis_keywords if keyword in draft)
        
        if analysis_count < 5:
            issues.append(f"æ·±åº¦åˆ†æä¸è¶³ï¼ˆä»…{analysis_count}å¤„åˆ†æè¯ï¼‰ï¼Œå»ºè®®å¢åŠ å› æœåˆ†æ")
        
        if issues:
            print(f"âš ï¸ å‘ç° {len(issues)} ä¸ªå¯ä¼˜åŒ–ç‚¹:")
            for issue in issues:
                print(f"  - {issue}")
            return draft, issues
        else:
            print("âœ… å®¡æ ¸é€šè¿‡ï¼Œå†…å®¹è´¨é‡è‰¯å¥½")
            return draft, []

    def _quality_check(self, draft):
        """
        ğŸ”¥ Step 7: è´¨é‡å®¡æ ¸ï¼ˆå››é‡æ£€æŸ¥ï¼‰
        æ£€æŸ¥è¯­ä¹‰é‡å¤ã€é€»è¾‘æ¼æ´ã€äº‹å®å‡†ç¡®ã€æ—¶é—´æ­£ç¡®
        """
        print("ğŸ” Step 7: è´¨é‡å®¡æ ¸...")
        
        issues = []
        char_count = len(draft)
        
        # 1. è¯­ä¹‰é‡å¤æ£€æŸ¥ï¼ˆç®€å•ç‰ˆï¼‰
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', draft)
        unique_sentences = set(sentences)
        if len(sentences) - len(unique_sentences) > 3:
            issues.append("æ£€æµ‹åˆ°è¾ƒå¤šé‡å¤å¥å­")
        
        # 2. å­—æ•°æ£€æŸ¥ï¼ˆæ›´ä¸¥æ ¼çš„æ ‡å‡†ï¼‰
        print(f"ğŸ“Š å½“å‰å­—æ•°: {char_count} å­—")
        if char_count < 800:
            issues.append(f"å†…å®¹è¿‡çŸ­ï¼ˆ{char_count}å­—ï¼‰ï¼Œç›®æ ‡è‡³å°‘1500å­—")
        elif char_count >= 800 and char_count < 1200:
            print(f"âš ï¸ å­—æ•°åå°‘ï¼ˆ{char_count}å­—ï¼‰ï¼Œç†æƒ³å€¼1500-2500å­—")
        elif char_count > 3500:
            issues.append(f"å†…å®¹è¿‡é•¿ï¼ˆ{char_count}å­—ï¼‰ï¼Œå»ºè®®æ§åˆ¶åœ¨2500å­—ä»¥å†…")
        else:
            print(f"âœ… å­—æ•°åˆæ ¼ï¼ˆ{char_count}å­—ï¼‰")
        
        # 3. æ£€æŸ¥æ˜¯å¦æœ‰å®è´¨å†…å®¹
        if "åˆ†æ" not in draft and "å½±å“" not in draft and "åŸå› " not in draft:
            issues.append("ç¼ºå°‘æ·±åº¦åˆ†æ")
        
        if issues:
            print(f"âš ï¸ å‘ç°é—®é¢˜: {', '.join(issues)}")
            return False, issues
        else:
            print("âœ… è´¨é‡å®¡æ ¸é€šè¿‡")
            return True, []

    def fetch_news_and_analyze(self):
        """
        ğŸ”¥ v2.2 - 15æ­¥ä¸“ä¸šå·¥ä½œæµç¨‹ï¼ˆå®Œæ•´ç‰ˆï¼‰
        """
        if not self.tavily: 
            return None, "ç¼ºå°‘ Tavily Key", False
        
        print("\n" + "="*60)
        print("ğŸ™ï¸ åŠ å¯†å¤§æ¼‚äº® - 15æ­¥ä¸“ä¸šå†…å®¹ç”Ÿäº§æµç¨‹ v2.2")
        print("="*60)
        
        # ğŸ”¥ Step 1: ç¡®è®¤å½“å‰å‡†ç¡®æ—¶é—´
        now = datetime.datetime.now()
        today_str = now.strftime("%Y-%m-%d")
        time_str = now.strftime("%H:%M:%S")
        weekday_cn = ['ä¸€', 'äºŒ', 'ä¸‰', 'å››', 'äº”', 'å…­', 'æ—¥'][now.weekday()]
        
        print(f"\nâ° Step 1: ç¡®è®¤å½“å‰æ—¶é—´")
        print(f"ğŸ“… ä»Šå¤©æ˜¯ {now.year}å¹´{now.month}æœˆ{now.day}æ—¥ æ˜ŸæœŸ{weekday_cn} {time_str}")
        print(f"ğŸŒ æ—¶åŒº: UTC+8 (åŒ—äº¬æ—¶é—´)")
        
        # ğŸ”¥ Step 2: äº†è§£é¢‘é“å®šä½å’Œå†å²æ•°æ®
        print(f"\nğŸ“Š Step 2: é¢‘é“å®šä½åˆ†æ")
        print(f"ğŸ“Œ é¢‘é“ä¸»é¢˜: {self.topic}")
        print(f"ğŸ¯ äººè®¾å®šä½: ä¸“ä¸šåŠ å¯†è´§å¸åˆ†æå¸ˆ")
        print(f"ğŸ“š æ¡†æ¶åº“: {len(self.frameworks)} ç§åˆ†ææ¡†æ¶")
        print(f"ğŸ”„ æœ€è¿‘ä½¿ç”¨æ¡†æ¶: {', '.join(self.recent_frameworks[-3:]) if self.recent_frameworks else 'æ— '}")
        
        # ğŸ”¥ Step 3: å®æ—¶è¿½è¸ªçƒ­ç‚¹
        print(f"\nğŸ” Step 3: å®æ—¶çƒ­ç‚¹è¿½è¸ª ({today_str})")
        
        domain_list = [d.strip() for d in self.target_domains.split(",") if d.strip()]
        
        try:
            # å¢å¼ºæœç´¢æŸ¥è¯¢ï¼ŒåŒ…å«æ˜ç¡®çš„æ—¶é—´ä¸Šä¸‹æ–‡
            response = self.tavily.search(
                query=f"crypto blockchain {self.topic} latest breaking news {today_str} today",
                search_depth="advanced",
                include_domains=domain_list if domain_list else None,
                max_results=15,  # å¢åŠ åˆ°15æ¡ï¼Œæ–¹ä¾¿ç­›é€‰
                days=1
            )
            results = response.get("results", [])
            print(f"âœ… æœç´¢åˆ° {len(results)} æ¡å€™é€‰æ–°é—»")
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return None, f"æœç´¢å¤±è´¥: {e}", False

        # è®¡ç®—çˆ†ç«æ½œåŠ›å¹¶æ’åº
        scored_results = []
        for item in results:
            score = self._calculate_viral_potential(item)
            scored_results.append((score, item))
        
        scored_results.sort(reverse=True, key=lambda x: x[0])
        print(f"ğŸ“Š çˆ†ç«æ½œåŠ›æ’åºå®Œæˆï¼ŒTop1å¾—åˆ†: {scored_results[0][0] if scored_results else 0}")
        
        # ç­›é€‰æœªè®²è¿‡çš„æ–°é—»
        selected_news = None
        selected_framework = None
        
        # ğŸ”¥ Step 4: é€‰é¢˜å»é‡æ£€æŸ¥ï¼ˆ60%è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼ï¼‰
        print(f"\nğŸš« Step 4: é€‰é¢˜å»é‡æ£€æŸ¥ï¼ˆ24å°æ—¶å†…ï¼Œ60%é˜ˆå€¼ï¼‰")
        
        for score, item in scored_results:
            # ğŸ”¥ FIX: å…¼å®¹ä¸åŒå­—æ®µå
            title = item.get('title') or item.get('name') or ''
            if not title:
                print("âš ï¸ å‘ç°æ— æ ‡é¢˜æ–°é—»ï¼Œè·³è¿‡")
                continue
            
            # ä½¿ç”¨æ–°çš„è¯­ä¹‰å»é‡æ£€æŸ¥
            is_duplicate = self._check_semantic_duplication(title, threshold=0.6, time_window_hours=24)
            
            if not is_duplicate and not self._check_duplication(title):
                selected_news = item
                
                # ğŸ”¥ Step 5: æ™ºèƒ½åŒ¹é…åˆ†ææ¡†æ¶
                print(f"\nğŸ“ˆ Step 5: æ™ºèƒ½æ¡†æ¶åŒ¹é…")
                selected_framework = self._match_framework(item)
                print(f"ğŸ¯ åˆæ­¥åŒ¹é… â†’ {selected_framework} ({self.frameworks[selected_framework]['name']})")
                
                # ğŸ”¥ Step 6: æ¡†æ¶å¤šæ ·æ€§æ£€æŸ¥
                print(f"\nğŸ”„ Step 6: æ¡†æ¶å¤šæ ·æ€§æ£€æŸ¥")
                selected_framework = self._check_framework_diversity(selected_framework)
                
                print(f"âœ… é€‰ä¸­å¤´æ¡: {title[:50]}...")
                print(f"âœ… æœ€ç»ˆæ¡†æ¶: {selected_framework} ({self.frameworks[selected_framework]['name']})")
                break
            else:
                print(f"â­ï¸ è·³è¿‡é‡å¤è¯é¢˜: {title[:40]}...")
        
        # æ²¡æ–°é—» â†’ å¯ç”¨ CMS å¤‡ç”¨åº“
        if not selected_news:
            print("âš ï¸ æ— æœ€æ–°é«˜ä»·å€¼æ–°é—»æˆ–éƒ½å·²è®²è¿‡ï¼Œå¯ç”¨å¤‡ç”¨è¯é¢˜åº“...")
            import random
            backup = random.choice(self.backup_topics) if self.backup_topics else "æ¯”ç‰¹å¸å»ä¸­å¿ƒåŒ–ç²¾ç¥ç§‘æ™®"
            print(f"ğŸ“š ä½¿ç”¨å¤‡ç”¨è¯é¢˜: {backup}")
            return backup, None, True

        # ğŸ”¥ Step 7: è®¾è®¡æ–‡ç« å¤§çº²
        title = selected_news.get('title') or selected_news.get('name') or ''
        outline = self._design_outline(title, selected_framework, selected_news)
        
        # ğŸ”¥ Step 8: å¹¿æ³›æ”¶é›†è¯æ®ç´ æ
        print(f"\nğŸ“š Step 8: å¹¿æ³›æ”¶é›†è¯æ®ç´ æ")
        evidence = self._collect_evidence(self.topic, selected_news)
        
        # ğŸ”¥ Step 9: ä¸¥æ ¼ç­›é€‰ç´ æï¼ˆå·²åœ¨_collect_evidenceä¸­å®ç°ï¼‰
        print(f"\nâš¡ Step 9: ä¸¥æ ¼ç­›é€‰ç´ æï¼ˆå®Œæˆï¼‰")
        
        # ğŸ”¥ Step 10: æŒ‰æ¡†æ¶ç»„ç»‡ç´ æ
        print(f"\nğŸ“– Step 10: æŒ‰æ¡†æ¶ç»„ç»‡ç´ æ")
        organized_content = self._organize_content(evidence, selected_framework, selected_news)
        
        # ğŸ”¥ Step 11: éªŒè¯ç´ ææ—¶é—´çº¿
        all_evidence = organized_content.get('æ”¯æ’‘è¯æ®', []) + organized_content.get('åé©³è¯æ®', [])
        verified_evidence = self._verify_evidence_timeline(all_evidence)
        organized_content['æ”¯æ’‘è¯æ®'] = [e for e in organized_content['æ”¯æ’‘è¯æ®'] if e in verified_evidence]
        
        # ğŸ”¥ Step 13: æ’°å†™æ–‡æ¡ˆï¼ˆåŸStep 8-10ï¼‰
        print("\nâœï¸ Step 13: AIæ·±åº¦æ’°å†™æ–‡æ¡ˆ...")
        
        framework_info = self.frameworks[selected_framework]
        
        # ğŸ”¥ FIX: å…¼å®¹ä¸åŒå­—æ®µå
        title = selected_news.get('title') or selected_news.get('name') or ''
        content_text = selected_news.get('content') or selected_news.get('snippet') or selected_news.get('description') or ''
        url = selected_news.get('url') or ''
        
        # ğŸ”¥ æ ¸å¿ƒ Prompt - v2.2 15æ­¥ä¸“ä¸šæµç¨‹ç‰ˆï¼ˆå¢å¼ºæ—¶é—´ä¸Šä¸‹æ–‡ï¼‰
        prompt = f"""
{self.persona}

â° **å½“å‰æ—¶é—´ä¸Šä¸‹æ–‡**ï¼š
ä»Šå¤©æ˜¯ {now.year}å¹´{now.month}æœˆ{now.day}æ—¥ æ˜ŸæœŸ{weekday_cn} {time_str}
æ—¶åŒºï¼šUTC+8 (åŒ—äº¬æ—¶é—´)

é‡è¦ï¼šä½ çš„åˆ†æå¿…é¡»åŸºäºæœ€æ–°æ—¶é—´ï¼Œä½¿ç”¨"ä»Šå¤©"ã€"æœ€è¿‘"ã€"åˆšåˆš"ç­‰è¯æ—¶è¦å‡†ç¡®ã€‚

ã€åˆ†æä»»åŠ¡ã€‘
ä½ æ­£åœ¨ä½¿ç”¨ **{framework_info['name']}** è¿›è¡Œæ·±åº¦åˆ†æã€‚

ã€å†…å®¹å¤§çº²ã€‘ï¼ˆä¸¥æ ¼éµå¾ªï¼‰
{chr(10).join([f"- å¼•è¨€ï¼š{outline['å¼•è¨€']['desc']} ({outline['å¼•è¨€']['target_chars']}å­—)"] + 
              [f"- {s['section']}ï¼š{s['desc']} ({s['target_chars']}å­—)" for s in outline['ä¸»ä½“']] +
              [f"- ç»“è®ºï¼š{outline['ç»“è®º']['desc']} ({outline['ç»“è®º']['target_chars']}å­—)"])}

æ¡†æ¶ç»“æ„: {framework_info['structure']}

ã€åŸå§‹æ–°é—»ã€‘
æ ‡é¢˜ï¼š{title}
å†…å®¹ï¼š{content_text}
æ¥æºï¼š{url}

ã€æ”¯æ’‘è¯æ®ã€‘
{chr(10).join([f"- {e['æ ‡é¢˜']}" for e in organized_content['æ”¯æ’‘è¯æ®']])}

ã€åˆ›ä½œè¦æ±‚ - ä¸¥æ ¼æ‰§è¡Œã€‘

1. **æ¡†æ¶åº”ç”¨**ï¼š
   - ä¸¥æ ¼æŒ‰ç…§ {selected_framework} æ¡†æ¶çš„ç»“æ„å±•å¼€
   - æ¯ä¸ªç¯èŠ‚éƒ½è¦æœ‰å®è´¨æ€§åˆ†æï¼Œä¸æ˜¯ç®€å•ç½—åˆ—
   - é€»è¾‘é“¾æ¡è¦å®Œæ•´ï¼Œç¯ç¯ç›¸æ‰£

2. **æ·±åº¦æŒ–æ˜**ï¼š
   - ä¸èƒ½åªå¤è¿°æ–°é—»ï¼Œå¿…é¡»æœ‰ç‹¬åˆ°è§è§£
   - æŒ–æ˜èƒŒåçš„æ·±å±‚åŸå› å’Œå½±å“
   - æå‡ºæœ‰ä»·å€¼çš„é¢„æµ‹æˆ–å»ºè®®
   - æ¯ä¸ªåˆ†æç‚¹è‡³å°‘å±•å¼€3-5å¥è¯ï¼Œä¸è¦ä¸€ç¬”å¸¦è¿‡
   - ç”¨å…·ä½“æ¡ˆä¾‹å’Œæ•°æ®æ”¯æ’‘ä½ çš„è§‚ç‚¹

3. **å£è¯­åŒ–è¡¨è¾¾**ï¼š
   - å¥å­è¦çŸ­ï¼Œå¹³å‡15å­—ä»¥å†…
   - åƒå’Œæœ‹å‹èŠå¤©ä¸€æ ·è‡ªç„¶
   - é€‚åˆå¥³æ€§ä¸»æŒäººçš„è¯­æ°”
   - å¸¦ç‚¹å¹½é»˜å’Œä¸ªæ€§

4. **å†…å®¹æ§åˆ¶**ï¼š
   - å­—æ•°ï¼š1500-2500å­—ï¼ˆç›®æ ‡8-15åˆ†é’Ÿæ’­æŠ¥æ—¶é•¿ï¼‰
   - èŠ‚å¥ï¼šæœ‰å¿«æœ‰æ…¢ï¼Œæœ‰é‡ç‚¹æœ‰å±•å¼€
   - ç»“æ„ï¼šæ¸…æ™°çš„å¼€å¤´ã€ä¸­é—´ã€ç»“å°¾
   - æ·±åº¦ï¼šå……åˆ†å±•å¼€æ¯ä¸ªåˆ†æç‚¹ï¼Œä¸è¦ç®€ç•¥æ¦‚æ‹¬

5. **ä¸¥ç¦äº‹é¡¹**ï¼š
   - ğŸš« ä¸è¦è¾“å‡ºä»»ä½•æ¡†æ¶åç§°ã€å…ƒä¿¡æ¯ã€åˆ†æè¯´æ˜
   - ğŸš« ä¸è¦è¾“å‡º"æˆ‘é€‰æ‹©äº†XXæ¡†æ¶""ä½¿ç”¨XXåˆ†æ"ç­‰å…ƒè¯­è¨€
   - ğŸš« ä¸è¦è¾“å‡ºåºå·ã€æ ‡é¢˜ã€ç« èŠ‚å·ï¼ˆå¦‚"ä¸€ã€""1.""ç¬¬ä¸€éƒ¨åˆ†"ï¼‰
   - ğŸš« ä¸è¦è¾“å‡ºä»»ä½•è‹±æ–‡å­—æ¯ã€æ•°å­—ç¼–å·ã€ä»£ç ã€æ ‡è®°
   - ğŸš« ä¸è¦ç”¨"å¥½çš„""æ²¡é—®é¢˜""ç»¼ä¸Šæ‰€è¿°"ç­‰åºŸè¯
   - ğŸš« ä¸è¦å‡ºç°(éŸ³æ•ˆ)ã€[åŠ¨ä½œ]ã€ã€è¯´æ˜ã€‘ç­‰å‰§æœ¬æ ‡è®°
   - ğŸš« ä¸è¦ç”¨å¼•å·ã€æ‹¬å·ã€ä¹¦åå·ç­‰ä¸é€‚åˆæœ—è¯»çš„ç¬¦å·
   - âœ… åªç”¨é€—å·ã€å¥å·ã€é—®å·ã€æ„Ÿå¹å·
   - âœ… ç›´æ¥å¼€å§‹è®²æ•…äº‹ï¼Œåƒæ’­éŸ³å‘˜ä¸€æ ·è‡ªç„¶æµç•…

6. **è¯­æ°”é£æ ¼**ï¼š
   - ä¸“ä¸šä½†ä¸æ­»æ¿
   - çŠ€åˆ©ä½†ä¸åæ¿€
   - çŸ¥æ€§ä½†ä¸é«˜å†·
   - æœ‰è§‚ç‚¹æœ‰æ€åº¦

âš ï¸ **é‡è¦æé†’**ï¼š
- ç›®æ ‡å­—æ•°ï¼š1500-2500å­—ï¼ˆçº¦8-15åˆ†é’Ÿæ’­æŠ¥æ—¶é•¿ï¼‰
- å¦‚æœå­—æ•°ä¸è¶³1500å­—ï¼Œå°†è¢«é€€å›é‡å†™
- å……åˆ†å±•å¼€åˆ†æï¼Œä¸è¦ç®€ç•¥æ¦‚æ‹¬
- æ¯ä¸ªè®ºç‚¹éƒ½è¦æœ‰è¶³å¤Ÿçš„è®ºè¯æ”¯æ’‘

ğŸ”´ **ä¸¥æ ¼å­—æ•°è¦æ±‚ - å¿…é¡»éµå®ˆï¼**ï¼š
- æœ€å°‘1500å­—ï¼Œç†æƒ³2000å­—ä»¥ä¸Š
- æ¯ä¸ªæ¡†æ¶ç¯èŠ‚è‡³å°‘200-300å­—
- ä¸è¦å†™æˆç®€çŸ­çš„æ–°é—»ç¨¿ï¼Œè¦å†™æˆæ·±åº¦åˆ†æé•¿æ–‡
- å‚è€ƒç¤ºä¾‹ï¼šä¸€ä¸ªå®Œæ•´çš„åˆ†æåº”è¯¥åƒä¸€ç¯‡æ·±åº¦æŠ¥é“æ–‡ç« 

ğŸ’¡ **å¦‚ä½•è¾¾åˆ°å­—æ•°è¦æ±‚**ï¼š
1. æ¯ä¸ªè®ºç‚¹éƒ½è¦æœ‰ï¼šè§‚ç‚¹é™ˆè¿° + äº‹å®æ”¯æ’‘ + æ•°æ®å¼•ç”¨ + å½±å“åˆ†æ
2. å¤šç”¨å…·ä½“ä¾‹å­ï¼šä¸è¦è¯´"ä¼šæœ‰å½±å“"ï¼Œè¦è¯´"å…·ä½“ä¼šå¯¹XXå¸‚åœºé€ æˆXXå½±å“ï¼Œé¢„è®¡XX"
3. å±•å¼€æ—¶é—´çº¿ï¼šä¸è¦åªè¯´"å‘ç”Ÿäº†"ï¼Œè¦è¯´"ä½•æ—¶å¼€å§‹ã€å¦‚ä½•å‘å±•ã€ç°åœ¨çŠ¶å†µã€æœªæ¥èµ°å‘"
4. å¤šè§’åº¦åˆ†æï¼šä»æŠ•èµ„è€…ã€ç›‘ç®¡è€…ã€è¡Œä¸šå‚ä¸è€…ç­‰å¤šä¸ªè§†è§’åˆ†æ

ğŸ™ï¸ **æœ€ç»ˆè¾“å‡ºæ ¼å¼è¦æ±‚**ï¼š
- ç›´æ¥è¾“å‡ºæ’­éŸ³ç¨¿ï¼Œç¬¬ä¸€ä¸ªå­—å°±æ˜¯æ­£æ–‡å†…å®¹
- ä¸è¦ä»»ä½•æ ‡é¢˜ã€åºå·ã€ç« èŠ‚ã€æ¡†æ¶è¯´æ˜
- ä¸è¦ä»»ä½•è‹±æ–‡ã€æ•°å­—æ ‡è®°ã€ä»£ç æ ¼å¼
- åƒæ–°é—»ä¸»æ’­ä¸€æ ·ï¼Œç›´æ¥å¼€å§‹è®²è¿°
- æµç•…è‡ªç„¶ï¼Œå¬ä¼—èƒ½ç›´æ¥ç†è§£

ç¤ºä¾‹å¼€å¤´ï¼ˆæ­£ç¡®ï¼‰ï¼š
"æ¯”ç‰¹å¸ä»·æ ¼ä»Šå¤©çªç ´äº”ä¸‡ç¾å…ƒå¤§å…³ï¼Œè¿™æ˜¯ç»§å»å¹´åä¸€æœˆä»¥æ¥çš„é¦–æ¬¡çªç ´..."

ç¤ºä¾‹å¼€å¤´ï¼ˆé”™è¯¯ï¼‰ï¼š
"âŒ ä¸€ã€äº‹ä»¶æ¦‚è¿° âŒ"
"âŒ ã€5W1Hæ¡†æ¶åˆ†æã€‘ âŒ"
"âŒ 1. Background âŒ"

ç°åœ¨å¼€å§‹åˆ›ä½œï¼Œç›´æ¥è¾“å‡ºçº¯æ–‡æ¡ˆæ­£æ–‡ï¼ˆè‡³å°‘1500å­—ï¼Œæ— ä»»ä½•æ ‡è®°ï¼‰ï¼š
"""
        
        try:
            # ğŸ”¥ å¤šæ¬¡å°è¯•æœºåˆ¶ï¼šç¡®ä¿ç”Ÿæˆé«˜è´¨é‡é•¿å†…å®¹
            max_attempts = 3
            best_script = None
            best_char_count = 0
            
            for attempt in range(max_attempts):
                print(f"ğŸ¨ ç¬¬ {attempt + 1} æ¬¡ç”Ÿæˆ..." if attempt > 0 else "ğŸ¨ å¼€å§‹ç”Ÿæˆå†…å®¹...")
                
                # æ ¹æ®å°è¯•æ¬¡æ•°è°ƒæ•´ prompt
                if attempt == 0:
                    current_prompt = prompt
                elif attempt == 1:
                    # ç¬¬äºŒæ¬¡ï¼šå¼ºè°ƒå­—æ•°è¦æ±‚
                    current_prompt = prompt.replace(
                        "ç°åœ¨å¼€å§‹åˆ›ä½œï¼Œç›´æ¥è¾“å‡ºæ–‡æ¡ˆæ­£æ–‡ï¼ˆè®°ä½ï¼šè‡³å°‘1500å­—ï¼ï¼‰ï¼š",
                        "ğŸš¨ğŸš¨ğŸš¨ ä¸Šä¸€æ¬¡ç”Ÿæˆå¤±è´¥ï¼šå­—æ•°ä¸¥é‡ä¸è¶³ï¼ğŸš¨ğŸš¨ğŸš¨\n\n" +
                        "ç¬¬äºŒæ¬¡å°è¯• - å¿…é¡»æ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š\n" +
                        "1. æœ€å°‘1500å­—ï¼Œç›®æ ‡2000å­—ä»¥ä¸Š\n" +
                        "2. æ¯ä¸ªæ¡†æ¶ç¯èŠ‚è¯¦ç»†å±•å¼€ï¼Œè‡³å°‘5-8å¥è¯\n" +
                        "3. ä¸è¦å†™ç®€çŸ­æ¦‚æ‹¬ï¼Œè¦å†™æ·±åº¦é•¿æ–‡\n" +
                        "4. å¤šç”¨å…·ä½“æ•°æ®ã€æ¡ˆä¾‹ã€æ—¶é—´çº¿\n\n" +
                        "ç°åœ¨å¼€å§‹åˆ›ä½œï¼Œè¾“å‡ºè‡³å°‘1500å­—çš„å®Œæ•´åˆ†æï¼š"
                    )
                else:
                    # ç¬¬ä¸‰æ¬¡ï¼šæœ€ä¸¥å‰è­¦å‘Š
                    current_prompt = prompt.replace(
                        "ç°åœ¨å¼€å§‹åˆ›ä½œï¼Œç›´æ¥è¾“å‡ºæ–‡æ¡ˆæ­£æ–‡ï¼ˆè®°ä½ï¼šè‡³å°‘1500å­—ï¼ï¼‰ï¼š",
                        "ğŸ”¥ğŸ”¥ğŸ”¥ æœ€åæœºä¼šï¼å‰ä¸¤æ¬¡éƒ½å¤±è´¥äº†ï¼ğŸ”¥ğŸ”¥ğŸ”¥\n\n" +
                        "ç¬¬ä¸‰æ¬¡å°è¯• - ç»ˆæè¦æ±‚ï¼š\n" +
                        "ğŸ“ å¿…é¡»ç”Ÿæˆè‡³å°‘1500å­—çš„æ·±åº¦åˆ†ææ–‡ç« \n" +
                        "ğŸ“ æ¯ä¸ªè®ºç‚¹å±•å¼€è‡³å°‘300å­—\n" +
                        "ğŸ“ åƒå†™è®ºæ–‡ä¸€æ ·è¯¦ç»†ã€åƒæŠ¥é“ä¸€æ ·æ·±å…¥\n" +
                        "ğŸ“ ä¸è¦ç®€ç•¥ã€ä¸è¦æ¦‚æ‹¬ã€ä¸è¦çœç•¥\n\n" +
                        "ç¤ºä¾‹é•¿åº¦å‚è€ƒï¼š\n" +
                        "- å¼€å¤´å¼•å…¥ï¼š200-300å­—\n" +
                        "- æ¯ä¸ªæ¡†æ¶ç¯èŠ‚ï¼š300-400å­— Ã— 4-5ä¸ªç¯èŠ‚ = 1200-2000å­—\n" +
                        "- ç»“å°¾æ€»ç»“ï¼š200-300å­—\n" +
                        "æ€»è®¡ï¼š1500-2500å­—\n\n" +
                        "ç«‹å³å¼€å§‹åˆ›ä½œå®Œæ•´çš„æ·±åº¦åˆ†æé•¿æ–‡ï¼š"
                    )
                
                raw_script = self.llm.invoke(current_prompt).content
                print(f"ğŸ“ åŸå§‹ç”Ÿæˆå­—æ•°: {len(raw_script)} å­—")
                
                clean_script = self._clean_text(raw_script)
                print(f"ğŸ§¹ æ¸…æ´—åå­—æ•°: {len(clean_script)} å­—")
                
                # ğŸ”¥ å¦‚æœæ¸…æ´—åæŸå¤±è¶…è¿‡30%ï¼Œä½¿ç”¨åŸå§‹ç‰ˆæœ¬
                if len(clean_script) < len(raw_script) * 0.7:
                    print(f"âš ï¸ æ¸…æ´—æŸå¤±è¿‡å¤šï¼ˆ{100 - len(clean_script)/len(raw_script)*100:.1f}%ï¼‰ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬")
                    clean_script = raw_script
                
                # è®°å½•æœ€ä½³ç»“æœ
                if len(clean_script) > best_char_count:
                    best_script = clean_script
                    best_char_count = len(clean_script)
                
                # ğŸ”¥ Step 14: æ¶¦è‰²å’Œå®¡æ ¸ä¼˜åŒ–
                polished_script, polish_issues = self._polish_and_audit(clean_script, outline)
                
                # åŸºç¡€è´¨é‡å®¡æ ¸
                passed, basic_issues = self._quality_check(polished_script)
                
                # ğŸ”¥ Step 15: å®¡æ ¸æœ‰é—®é¢˜ï¼Œå°±ä¿®æ”¹é—®é¢˜
                if passed and not polish_issues:
                    print(f"\nâœ… Step 15: å†…å®¹å®¡æ ¸é€šè¿‡ï¼")
                    print(f"ğŸ“Š æœ€ç»ˆå­—æ•°: {len(polished_script)} å­—")
                    print(f"â±ï¸ é¢„è®¡æ’­æŠ¥æ—¶é•¿: {len(polished_script)/3:.1f} ç§’ ({len(polished_script)/3/60:.1f} åˆ†é’Ÿ)")
                    print("="*60 + "\n")
                    return polished_script, None, False
                else:
                    all_issues = basic_issues + polish_issues
                    print(f"\nâš ï¸ Step 15: å‘ç°é—®é¢˜éœ€è¦ä¿®æ­£")
                    print(f"âŒ ç¬¬ {attempt + 1} æ¬¡ç”Ÿæˆæœªé€šè¿‡å®¡æ ¸: {', '.join(all_issues)}")
                    if attempt < max_attempts - 1:
                        print(f"ğŸ”„ å°†è¿›è¡Œç¬¬ {attempt + 2} æ¬¡å°è¯•ï¼Œé’ˆå¯¹æ€§æ”¹è¿›...")
                    
                    # æ›´æ–°æœ€ä½³ç»“æœ
                    if len(polished_script) > best_char_count:
                        best_script = polished_script
                        best_char_count = len(polished_script)
            
            # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œè¿”å›æœ€ä½³ç»“æœ
            print(f"\nâš ï¸ {max_attempts} æ¬¡å°è¯•åï¼Œä½¿ç”¨æœ€ä½³ç»“æœï¼ˆ{best_char_count}å­—ï¼‰")
            print("="*60 + "\n")
            return best_script if best_script else clean_script, None, False
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            return None, f"ç”Ÿæˆå¤±è´¥: {e}", False
